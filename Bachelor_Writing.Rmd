---
title: "Kandidat Skrivandet"
author: "Anton Holm"
date: '2020-02-24'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(jpeg)
library(tidyverse)
library(kableExtra)

#Loading a plot showcasing one example of the simulation
load("Rdata_Files/example_graph.Rda")

#Loading Beta plots
load("Rdata_Files/Plot_Low_Individual_Variance.Rda")
load("Rdata_Files/Plot_Medium_Individual_Variance.Rda")
load("Rdata_Files/Plot_High_Individual_Variance.Rda")

#Loading Summary Tables
load("Code_Chunks/Table_LOQ30_Slope1.01.Rda")
load("Code_Chunks/Table_LOQ60_Slope1.01.Rda")
load("Code_Chunks/Table_LOQ30_Slope1.05.Rda")
load("Code_Chunks/Table_LOQ60_Slope1.05.Rda")

#Loading Application Tables
load("Code_Chunks/df_PB_Herring.Rda")
load("Code_Chunks/df_NI_Herring.Rda")
load("Code_Chunks/df_CR_Herring.Rda")


load("Rdata_Files/substitution_df_application.Rda")
```

## Abstrakt

## Introduction
- Maybe add graph to show how censored data looks like? So 2 graphs, one how it actually is, one how it's reported.
- Show that the museum data is log-linear
- Explain censored data

```{r}

```

## Background
At the Swedish Museum of Natural History, the Department of Environmental Research and Monitoring in a joint effort with other departments conducts statistical research of environmental toxicants as part of the National Swedish Contaminant Programme in marine biota. One of the programs conducted regards analysing long term time trends of several toxins in Swedish waters and to estimate the rate of change. The models used to analyse such time trends are at the moment surprisingly elemental and disregards much of the data collected. One of the more common, but nonetheless crucial oversights, concerns building models and drawing conclusions from fabricated data due to data being censored.

## Data
The report from Bignert at al (2017) explains much of the data sampling. The data comes from several sampling areas regarded as locally uncontaminated. Several species of fish, as well as guillemot eggs and blue mussels, are collected from different sampling areas each year. When collected, a constant number of 10-12 speciemens independent of each other are analysed for a large number of toxins. For some species, the analysis is done for pooled samples containing a number of speciemens in each pool. To reduce the between-years variation, each sampling area tries to analyse speciemens of the same sex and age. However, the variation can not be reduced to zero and other parameters effects the variation such as fat content and local discharges as an example. The concentration between each fish will also contain noise, hence the data sampled will have variation between years as well as within years.

As a result of test equipments not being able to detect small enough quantities of toxins, a portion of the data is reported as *below the limit of quantification (LOQ)*. This portion of the data is reported as the LOQ divided by the square root of 2.

Due to biological properies such as size and fat tissues being able to effect the concentration of toxins and these attributes being effected by sampling site, this thesis will analyse sampling areas individually.

Bignert at al (2017) uses log-linear regression analysis, hence the data is assumed to follow a log-linear distribution.


## Common Errors
One of the most common error being made when analysing censored data is fabricating. The analysists simply substitute the non-detects with a fraction (often one half) of the quantitive- or detetection limit. A simulation were made by Helsel (2006) showcasing that this method produces lousy estimates of statistics and have the potential to not only overlook patterns in the data, but also impose it's own fabricated patterns. This could result in a goverment investing millions to clean a lake of toxins after a report displaying an increase in concentrations of a certain metal in fish when in fact, there were no such pattern to begin with. The reverse is even more terrifying, obtaining a report showing no significant increase in concentration, when indeed the concentration of said metal have been increasing for years. Causes of an increase in concentration have been missed, remediations goes undone and the health of humans and the ecosystem is unnecessarily endangered. There are plenty more mistakes commonly being made when handling censored data including misinterpreting an improvement in measuring technique for a decrease in non-detects. However, this will not be discussed in detail in this thesis.



# Theory
When working with censored data, the non-detects can't be looked at as having a specific value. Instead, a combination of the information of the proportion of non-detects with the numerical values of the uncensored observations gives a better understanding of the data. Assuming a distribution for the data above and below the reported limit in combination with the above mentioned information gives a foundation to work with maximum likelihood estimates (MLE). In a study of Chung (1990) regarding regression analysis of geochemical data with non-detects, it was shown that MLE gave a much better estimation for the true value of the slope coefficient than any of the substitution values (0, 0.1, ..., 1 times the detection limit). Regression analysis for censored data is being used in many fields, including but not limited to, medical statistics as used by Lee and Go (1997) and in economics where Chay and Honore (1998) used MLE regression on right-censored data to model incomes. However, for left-censored data where the residuals is assumed to follow a normal distribution, the MLE regression is sometimes mentioned as Tobit analysis after the famous economist James Tobin. For the particular data from the Museum of Natural History, the use of Tobit regression models can serve useful to handle the censoring while the use of a Linear Mixed-Effect Model (LMM) will deal with the fact that data contains variation both within and between years.

## CDF of a linear regression model
Consider a normal simple linear regression model
$$
y_i = x_i \beta + \epsilon_i, \; \epsilon_i \sim N(0,\sigma^2)
$$

were $y_i$ is the response variable, $x_i$ the explanatory variable, $\beta$ an effect parameter and $\epsilon_i$ the error term.
It's then easy to find the cumulative distribution function (CDF) for this model.

$$
F(y_i) = P(x_i\beta+\epsilon_i\leq y_i) = P(\frac{\epsilon_i}{\sigma}\leq \frac{1}{\sigma}(y_i-x_i\beta)) = \Phi[\frac{1}{\sigma}(y_i-x_i\beta)]
$$

where $\Phi(\cdot)$ is the CDF for a standard normal variable. The probability density function (PDF) is further given by $f(y_i)=\frac{dF(y_i)}{dy_i}$.

## Linear mixed-effects model
$\mathbf{**}$Can aggregate data. Take mean of each group => the avg data points are now independent: Less noise but disregard a lot of data
Can do regression on each group => a lot of noise but takes all data
LMM somewhere in between$\mathbf{**}$

Mixed models are an extension of normal models where random effects are integrated. A linear mixed model is an extension of mixed models where both the fixed and random effects take place linearly in the model. The random effects can be observed as additional error terms in the model. Following the notation of Pinheiro and Bates (2000) the linear mixed model for a single level of grouping, as described by Laird and Ware (1982), can be expressed as

$$
\mathbf{y_i} = \mathbf{X_i}\mathbf{\beta} + \mathbf{Z_i}\mathbf{b_i} + \mathbf{\epsilon_i}
$$

for $i = 1,...,M$. Here, $\mathbf{y_i}$ is the $n_i$ dimension respons vector for group i, $\mathbf{\beta}$ the $p$ dimensional vector of fixed-effect parameters, $\mathbf{b_i}$ the $q$ dimensional vector of random-effects, $\mathbf{X_i}$ a matrix with covariates of size $n_i$ x $p$, $\mathbf{Z_i}$ a design matrix of size $n_i$ x $q$ linking $\mathbf{b_i}$ to $\mathbf{y_i}$ and $\mathbf{\epsilon_i}$ an $n_i$ dimension vector of error terms within group i with $\mathbf{b_i}\sim N(0,\Sigma)$, $\Sigma$ being the symmetrical, positive semi-definite $n_i$ x $n_i$ dimension covariance matrix and $\mathbf{\epsilon_i}\sim N(0,\sigma^2I)$, $I$ being the $n_i$ dimension vector of ones.









## Maximum Likelihood Estimation
One of the most interesting analysis to be made within regression analysis is what effect each covariate has on the response variable. This is represented by the unknown effect parametervector $\mathbf{\theta}$ ($\mathbf{\beta}$ in the model above), and thus something of great importance to be able to estimate. This is often done using Maximum Likelihood Estimation. For a response variable $\mathbf{Y}$ with observations $\mathbf{Y} = \mathbf{y}$ having a probobility mass or density function $f(\mathbf{y};\mathbf{\theta})$, depending on the observations $\mathbf{y}$ and $\mathbf{\theta} \in \mathbf{\Theta}$ being the often unknown parametervector taking values in the parameterspace $\mathbf{\Theta}$, the Likelihood Function is given by $L(\mathbf{\theta;\mathbf{y}}) = f(\mathbf{y};\mathbf{\theta})$. Using the definition of Held and BovÃ© (2014), the likelihood function is the probobility mass or density function of the observed data $\mathbf{y}$ viewed as a function of the paramatervector $\mathbf{\theta}$. The maximum likelihood estimate of $\mathbf{\theta}$ denoted as $\mathbf{\hat{\theta}}_{MLE}$ is then given as the parametervector maximising the likelihood function.


## Tobit Model
The Tobit model is characterized by the latent regression equation
$$
y_i^* = \mathbf{x_i}\cdot\mathbf{\beta} + \epsilon_i, \; \epsilon_i \sim N(0, \sigma^2)
$$

where $y_i^*$ is the laten dependent variable, $\mathbf{x_i}$ is a vector of covariates, $\mathbf{\beta}$ a vector of effect parameters and $\epsilon_i$ is the error term. Given this,  the observed dependent variable can be specified as:

$$
\begin{cases}
y_i = y_i^*, & y_i^* > y_L \\
y_i = y_L, & otherwise
\end{cases}
$$

with $y_L$ being the reporting limit. This leads us to the PDF of the Tobit model:


$$
f(y_i|\mathbf{x_i}) = \begin{cases}
f(y_i|\mathbf{x_i}) = 0, & y_i<y_L\\
f(y_L|\mathbf{x_i}) = P(y_i^* \leq y_L|\mathbf{x_i}), & y_i=y_L\\
 f(y_i|\mathbf{x_i})=f(y_i^*|\mathbf{x_i}), & y_i>y_L
\end{cases}
$$

Using the same method as for a normal simple linear regression model, we further deduce

$$
f(y_i|x_i)= \begin{cases}
0, & y_i<y_L \\
\Phi\bigg{(}\frac{y_L-\mathbf{x_i}\cdot\mathbf{\beta}}{\sigma}\bigg{)}, & y_i=y_L \\
\frac{1}{\sigma}\phi\bigg{(}\frac{y_i-\mathbf{x_i} \cdot \mathbf{\beta}}{\sigma}\bigg{)}, & y_i>y_L
\end{cases}
$$

where $\phi(\cdot)$ is the PDF of a standard normal distribution. Hence, the likelihood function for the Tobit model is:

$$
L = \underset{y_i=y_L}{\Pi} \Phi\bigg{(}\frac{y_L-\mathbf{x_i}\cdot\mathbf{\beta}}{\sigma}\bigg{)} \cdot \underset{y_i>y_L}{\Pi}\frac{1}{\sigma}\phi\bigg{(}\frac{y_i-\mathbf{x_i}\cdot\mathbf{\beta}}{\sigma}\bigg{)}
$$

## Case of the museum (Multivariate Normal-distribution)
Now, in the case of the analysis conducted by the Swedish Museum of Natural History, a Linear Mixed Tobit Model could be implemented. Regarding each year as a seperate group $t$ having $n_t$ speciemens. The between-year variance is the same for each speciemen in the same group while the within-year variance is the same for every speciemen through each year.

Hence, the model is

$$
\log(\mathbf{y_t}) = \mathbf{x_t} \cdot \mathbf{\beta} + \mathbf{z} \cdot \mathbf{e_t} + \mathbf{\epsilon}
$$

where $\mathbf{y_t}$ is the $n_t$ dimension response vector containing the measured concentration of a certain toxin, $\mathbf{x_t}$ a matrix of dimension $n_t$ x $2$ having a column of ones for the intercept and a column of the year of sampling, $\mathbf{\beta}$ the 2 dimensional vector of fixed effect parameters including the intercept, $\mathbf{z}$ an $n_t$ dimensional row vector of ones, $\mathbf{e_t}$ an $n_t$ dimensional vector of the random effect $e_t$ and $\mathbf{\epsilon}$ the $n_t$ dimensional vector with the within-years variance for each speciemen $\epsilon_i, i=1,2,...,n_i$. Further more, since $e_t \sim N(0,\sigma_t^2)$ and $\epsilon \sim N(0,\delta^2)$, the distribution of $\log(\mathbf{y_t})$ follows

$$
\log(\mathbf{y_t}) \sim N_{n_t}(\mathbf{x_t}\cdot \mathbf{\beta}, \mathbf{\Sigma})
$$

with $\mathbf{\Sigma} = (a_{ij})\in \mathbb{R}^{n_t \text{x} n_t}$ the covariance matrix where $(a_{ij}) = Cov (e + \epsilon_i, e +\epsilon_j)$.
Further calculations of the covariance gives

$$
Cov(e + \epsilon_i, e + \epsilon_j) = E[(e+\epsilon_i)(e+\epsilon_j)] - E[e+\epsilon_i]E[e+\epsilon_j]= E[\epsilon^2] = \delta^2
$$

for all $i,j$ such that $i\neq j$ since $E[e]=E[\epsilon_k]=0$ for all $k$. In addition, $(a_{ij}) = Var(e +\epsilon_i) = \sigma^2 + \delta^2$ when $i=j$.

Following the method above used to derive the CDF of a linear regression model, the CDF of the model in question can also be derived. First of all, the fact that observations can be censored must be taken into consideration. This is done by partioning the data into censored and non-censored components

$$
\mathbf{y_t}=
    \begin{bmatrix}
           \mathbf{y_t^o} \\
           \mathbf{y_t^c} \end{bmatrix}
           \mathbf{x_t} =  \begin{bmatrix}
           \mathbf{x_t^o} \\
           \mathbf{x_t^c} \end{bmatrix}
         \mathbf{\Sigma_t} = \begin{bmatrix}
           \mathbf{\Sigma_t^{oo}} & \mathbf{\Sigma_t^{oc}}\\
           \mathbf{\Sigma_t^{oc^{T}}} & \mathbf{\Sigma_t^{cc}}
         \end{bmatrix}
$$

where $\mathbf{y_t^o}$ is the $n_t^o$ vector of all the observed, non-censored values and $\mathbf{y_t^c}$ the $n_t^c$ vector of all censored observations, the same following for $\mathbf{x_t}$ being  partioned into a $n_t^o \, \text{x} \, 2$ matrix and a $n_t^c \, \text{x} \, 2$ matrix while $\mathbf{\Sigma_t^{oo}}$ and $\mathbf{\Sigma_t^{cc}}$ is the matrix of variances and covariances between all observed values and censored values respectively and $\mathbf{\Sigma_t^{oc}}=\mathbf{\Sigma_t^{co^{T}}}$ being the matrix of covariances between non-censored and censored observations. It follows that $\mathbf{y_t^o}$ has a multivariate normal distribution with PDF $f_{\mathbf{y_i}^o}$. Using the properties of the multivariate normal distribution, following Eaton (1983), the conditional distribution of $y_t^c|y_t^o$ is also multivariate normally distributed with mean and variance as follows

$$
\mu_t^{c|o} = \mathbf{x}_t^c\mathbf{\beta} + \mathbf{\Sigma_t^{co}}\mathbf{\Sigma_t^{{oo}^{-1}}}(\mathbf{y_t^o}-\mathbf{x_t^o\beta}), \;\;\;\; \mathbf{\Sigma_t^{c|o}} = \mathbf{\Sigma_t^{cc}}-\mathbf{\Sigma_t^{co}}\mathbf{\Sigma_t^{{oo}^{-1}}}\mathbf{\Sigma_t^{co^{T}}}
$$

here $\Sigma_t^{oo^{-1}}$ is the inverse of $\Sigma_t^{oo}$. Denote $\phi_t^{c|o}(\cdot)$ as the PDF of the conditional distribution function of $y_t^c$ given $y_t^o$ and $\mathbf{c_t}$ the $n_t^c$ vector where $c_{tj}$ is the censoring threshold for the $j^{th}$ censored outcome. Now, since all $\mathbf{y_t}$ are independent, using the methods of previous sections and the definition of the conditional proboility density function (Held and BovÃ©, p.321), the likelihood function can be written as

$$
L(\mathbf{\beta};\mathbf{y_t}) = \underset{t}{\Pi} f_{\mathbf{y_t}^o}(\mathbf{y_t}^o|\mathbf{\beta})\cdot \phi_t^{c|o}(\mathbf{c_t}|\mathbf{\beta})
$$

which given the PDF of a multivariate normal distributed variable gives

$$
\begin{aligned}
& L(\mathbf{\beta};\mathbf{y_t}) = \underset{t}{\Pi} \frac{1}{\sqrt{(2\pi)^{n_t^o}|\mathbf{\Sigma_t}^{oo}|}}\cdot exp\bigg{\{}-\frac{1}{2}(\mathbf{y_t}^o-\mathbf{x_t}^o\mathbf{\beta})^T\mathbf{\Sigma}_t^{oo^{-1}}(\mathbf{y_t}^o-\mathbf{x_t}^o\mathbf{\beta})\bigg{\}} \cdot \\
& \int_{-\infty}^{n_{t1}}\int_{-\infty}^{n_{t2}}\cdots\int_{-\infty}^{n_{tn_t^c}} \frac{1}{\sqrt{(2\pi)^{n_t^c}|\mathbf{\Sigma_t}^{c|o}|}}\cdot exp \bigg{\{}-\frac{1}{2}(\mathbf{z}-\mathbf{\mu^{c|o}})^T\mathbf{\Sigma}_t^{c|o^{-1}}(\mathbf{z}-\mathbf{\mu^{c|o}})\bigg{\}}
\end{aligned}
$$

Considering the museum is working on analysing timetrends and estimating the rate of change, what is of interest now is just that, to estimate the rate of change or in other words, to find the estimate for the parametervector $\mathbf{\beta}$. This is more often than not done by finding the root to the *score equation* $S(\mathbf{\beta}) = \frac{d}{d\mathbf{\beta}}L(\mathbf{\beta})$ and making sure that the solution is a global maxima. To simplify the calculations, the *log-likelihood function* $l(\mathbf{\beta})=\log[L(\mathbf{\beta})]$ is often used instead of the likelihood function. In light of the fact that the natural logarithm is a monotone and injective function, the parametervector maximising $l(\mathbf{\beta})$ is the same parametervector maximising $L(\mathbf{\beta})$.

Now, due to the fact that the likelihood function acquired from the model of the museum being so complex whilst having censored observation, the maximum likelihood estimate is difficult, if not impossible, to find analytically. Therefor, a numerical approach is suggested as also suggested by Dempster, Laird and Rubin (1977), namely, the Expectation-Maximization algorithm, also called the EM-algorithm. 

#### EM-Algorithm
The EM algorithm is an iterative method for estimating the MLE when the complete data-set is $Z=(X,Y)$ where $X$ is observed data while $Y$ is unobserved. The algorithm contains two steps, the Expectation-step and the Maximizing step, hence it's name. For each iteration, the algorithm produce an estimate $\mathbf{\theta}^{(i)}$ resulting in a sequence of estimates $\mathbf{\theta}^{(0)}, \mathbf{\theta}^{(1)},...,\mathbf{\theta}^{(p)}$ converging towards $\hat{\mathbf{\theta}}_{MLE}$, the MLE estimate of the parameter vector in question as $p$ tends towards infinity (Dempster et al., 1977). Although, it's not correct to say that the algorithm produce the same estimation as the MLE considering the fact that the algorithm will stop, either after some number of iterations decided before hand or when $|\mathbf{\theta}^{(i)} - \mathbf{\theta}^{(i-1)}| < \epsilon$ for some determined $\epsilon > 0$. Once again using the definition of the conditional probobility density function, we can write the joint pdf of $X$ and $Y$ as 

$$
f(\mathbf{x},\mathbf{y}) = f(\mathbf{y}|\mathbf{x}) f(\mathbf{x})
$$

and so following the derivations of Held and BovÃ© (2014) the log-likelihood can be expressed as,

$$
l(\mathbf{\theta};\mathbf{x},\mathbf{Y}) = l(\mathbf{\theta};\mathbf{Y}|\mathbf{x}) + l(\mathbf{\theta};\mathbf{x})
$$

where $\mathbf{y}$ is unobserved and hence exchanged by the random variable $\mathbf{Y}$. Now taking the expectation of this equation with regards to the complete data-set $\mathbf{Z}$ conditioned on the observed data $\mathbf{X}$ and the $i$:th estimate $\mathbf{\theta}^{(i)}$ we get

$$
E_{\mathbf{Z}}[l(\mathbf{\theta};\mathbf{x},\mathbf{Y}) ; \mathbf{\theta}^{(i)}] = E_\mathbf{Z}[l(\mathbf{\theta};\mathbf{Y}|\mathbf{x}); \mathbf{\theta}^{(i)}] + l(\mathbf{\theta};\mathbf{x})
$$

where we denote the left hand side as $Q(\mathbf{\theta}, \mathbf{\theta}^{(i)})$. The fact that $l(\mathbf{\theta};\mathbf{x})$ is left unchanges is due to it not depending on $\mathbf{Y}$. Knowing this, the EM-algorithm can now be explained in 3 steps:

1. Let $i = 0$ and $\mathbf{\theta}^{(i)}$ be the initial guess of the estimate and compute $Q(\mathbf{\theta}, \mathbf{\theta}^{(i)})$ called the E-step.
2. Maximize $Q(\mathbf{\theta}, \mathbf{\theta}^{(i)})$ with respect to $\mathbf{\theta}$ which yields $\mathbf{\theta}^{(i+1)}$, called the M-step.
3. Iterate step 1 and 2 by exhanging $\mathbf{\theta}^{(i)}$ with $\mathbf{\theta}^{(i+1)}$ in step 1 untill one of the mentioned reason to stop the algorithm has been reached.



## Simulation
The existence of bias for estimates where fabricated data were used have been evaluated by many others, see for example Thompson and Nelson (2003). El-Shaarawi and Esterby (1992) further showed that it's impossible to get unbiased estimates of the mean and standard deviation when using a single value replacing the censored observations while also showing that the bias is independent of sample size, and so what effects the bias is the proportion of censored values and the attributes for the distribution of the data. What is left to investigate is under what conditions one model is better than the other. A simulationstudy was therefore applied, trying to mimic the environmental setting of the museum as well as possible. Thus, a mixed linear model containing one centered covariate $X$ representing years ranging between $-5$ and $5$, and two error terms, $\epsilon$ and $b$, the former representing the noise for each individual specimen, the latter representing noise between-years, was used to investigate different conditions. The between-years variance are different for each year but othervise independent and the intercept was set to $0$. The sample size was set to $n_i=12$ samples for every year, the same as most of the studies used by the museum. Consequently the model assumed for the simulation was:

$$
\log(Y_{ij}) = X_{i}\beta + Zb_i + \epsilon, \; i=1,...,11, \; j= 1,...,n_i
$$

with $i$ being the index denoting the corresponding year and $j$ denoting the individual specimen for that year. Both error terms following a normaldistribution with mean $0$ and different variances for different scenarios. There are countless of scenarios to consider but this simulationstudy takes a closer look on four factors, namely

1. The proportion of censored data varied between $30 \%$ and $60 \%$ with all data being left-censored.
2. The slope of the regression line alternating between a yearly increase of $1 \%$ and $5 \%$ on the original scale
3. The two error terms $\epsilon$ and $b$ changed between small, medium and large for the noise of the individual specimen and betweel small and large for the between-year noise.

resulting in $24$ different scenarios. The limit of quantification was in other words put at the value representing $30 \%$ and $60 \%$ censored data if both the intercept and slope were to be put to $0$. Hence the proportion of censored data are affected by the slope. The exact values of the error terms were calculated using the methods of Helsel (2005) and the *NADA* packages in *R*, namely the *cenmle* function used on the data retrieved from the Swedish Museum of Natural History to calculate the standard deviations on individual and yearly level. One of the lower and higher noise of the individual specimen for a location was chosen aswell as the standard deviation between these two were chosen to be included in the simulationstudy. For the between-years noise, the lower value represent where most of the noise lied while the larger value represent some of the more extreme cases. For the noise of each individual specimen this resulted in a standard deviation of $0.05$, $0.5$ and $1.4$. Most of the locations in the data had standard deviations on individual level at somewhere between $0.05$ and $0.5$. The standard deviation for the noise between years were given by calculating the noise for each year seperately, choosing some of the lowest and highest value for each year resulting in the standard deviation ranging between $0.0007$ and $0.05417$ on the lower scale and between $1.044$ and $4.069$ for the larger scale. For most cases, the between-year variances were around the lower scale, however, considering some of the cases had higher variances, using a larger scale is also of interest.

For each of these scenarios, $100$ simulation were made in which the method of substituting censored observations with a fraction of the limit of quantification (in this case using the entity of LOQ/$\sqrt2$ to continue mimicing the museum) and the maximum likelihood method were both used. The data-sets were simulated using the model describe above and the R function *rlnorm* to simulate the error terms. The results from the model using fabricated data were retrieved using the base R function *lm* while the results for the maximum likelihood method were calculated using the *lmec* function from the package with the same name produced by Vaida and Liu (2009). For the EM-algorithm used in the lmec function to estimate effect parameters a cap of $20$ iterations were decided due to the immense time effort needed for the lmec function when using a data-set with high proportion of censored data. An example of the data obtained from one of the simulations can be seen in Figure 1. 

```{r, echo = FALSE}
example_graph
```

Figure 1 shows one of the simulated data-sets when computing at a censoring level of $60 \%$ with a slope implying a $5 \%$ yearly increase, where $X$ is to represent different years and the y-axis serve as an illustration of the logarithmic value of concentration of an arbritary metal found in a specimen. Seing next to no correlation between the covariate and the response is to be expected in an environment where changes happens slowly. However, small changes over a longer period of time might still have a huge impact as illuminated by Bignert et. al. (2017, pp. 46). 

Tables 1-4 shows a summary of the simulation grouped by proportion of censored values and the true value of the slope. The squared bias for each estimator was estimated using monte carlo methods. For each simulation $1,2,...,100$, the squared bias was calculated as $(\hat{\beta} - \beta)^2$ and the mean of this squared bias over all $100$ simulations were calculated for every scenario. The precision of the monte carlo method were calculated using the standard error of the estimated bias defined as the standard deviation of the bias divided by the sqaure root of the number of simulation, in this case $\sqrt{100}$.

The first thing that stands out, which might come as a surprice is the fact that whenever at least one of the errorterms are not set to a lower value, the Tobit model produce more or close to equally biased estimates than in the case of using substitution. However, when the errorterms have less influence, the Tobit model produced unbiased estimates while the substitution method, as shown by El-Shaarawi and Esterby (1992), still fails to produce unbiased estimates. Another interesting conclusion is in that when using substitution, the bias increase as the slope increase while the reverse seems to be true for the Tobit model except for the special case of holding both error terms and the proportion of censored values at a high level (compare Table 3 & 4). When alterning the proportion of censoring the Tobit model gives more biased estimates at higher proportions as to be expected considering there are less information while the substitution method does the same for the larger slope value and at the same time the reverse for a lower value of the slope. The Tobit model does however still give unbiased estimates at small noise at both of the proportion of censoring. 

**FÃ¶rklara Vilket konfidensintervall!!**
**Wald-CI: se(beta)^2 = (squared bias)/(simulations-1) **

When taking a closer look at the coverage of both methods the Tobit model clearly has a coverage around $95 \%$ in all cases. On the other hand, even though using substitution might have produced less bias in most cases, the coverage is not even close to the promised $95 \%$ for all scenarios. Anytime the errorterms are held at a low level, the coverage is close to zero. There are at the same time no clear patterns for any of the factors affecting the coverage of the Tobit model except for a slight decrease in coverage when the between-year errors are set to low over high. 

The variance of the estimator for the model of the museum is to no surprise much lower than that of the Tobit model considering the method of substitution. The variance of the estimators are obviously affected by the level of the errorterms. However, the effect is much clearer for the Tobit model than it is for the method of substitution. The inclination of the slope seems to have no major effect on the variance except for once again one special case for the Tobit model, when all factors are set to a high level (compare Table 3 & 4). What might be of more interest is the effect censoring has on the estimator. For the Tobit model, there is a clear increase in variance whenever the proportion of censoring is larger while the reverse, to no surprise, holds true when substituting values. Whenever the error terms are set to low, or the censoring level in combination with the slope both being high, the variance for the method of the museum is too low, resulting in too small confidence intervals.

The precision of the monte carlo estimates of the bias, as demonstrated by the standard error of the bias, is great when the noise is low. The standard error increase, meaning the precision decrease, when the noise gets more noticable. The precision for the Tobit model seem to decrease when the proportion of censoring increases which is to be expected, while the reverse is true for the Museum model.

```{r}
Table_LOQ30_Slope1.01
Table_LOQ30_Slope1.05
Table_LOQ60_Slope1.01
Table_LOQ60_Slope1.05
```

Figure **Skriv plot nummer** shows each simulated estimate of the slope for both methods plotted against eachother with Figure **Plot Nummer** treating each scenario when the individual noise is set to low, Figure **Plot Nummer** the case of a medium noise and Figure **Plot Nummer** when it is set to high. The first thing that jumps out is how big of an influence both errorterms have separately since altering just one of them from low to a higher value instantly results in much more biased estimates for both models. 

Another interesting fact is that for each and every scenario, the estimates of the Tobit model seem to center around the true value of the slope, having around the same proportion of estimates under the true value as over. However the same can not be said for the substitution model. Explicitly when the error terms have low effect and the slope is higher, not a single unbiased estimate is produced. It's also easy to see that when at least one of the error terms is set to higher than low in combination with a larger proportion of censoring, the majority of the estimation for the substitution method actually underestimate the slope, giving a result skewed towards lower values. At the same time, while the substitution method underestimates the value of the slope more often than the Tobit model, the Tobit model does miss by a lot more at some times. This might be one of the reasons for the higher bias of the Tobit model. 

Yet another sriking result is the fact that the proportion of censoring seems to have a big impact on the correlation between the estimates of the two models. More specific, whenever the proportion of censoring increase, the reverse goes for the correlation between the estimates. This is most likely an effect from the fact that the Tobit model keeps centering around the true slope value when the substitution model skews towards lower estimates. 


```{r}
Plot_Low_Individual_Variance
Plot_Medium_Individual_Variance
Plot_High_Individual_Variance
```



## Application

### Probabiliy plots and distribution assumptions
In the dataset used by the museum for their analysis of environmental toxins a large number of different metals were analysed. Three of which has the tendency of having a rather large proportion of censored values and are the three metals of most interest to analyse in this thesis. These metals are nickel (NI), lead (PB) and chromium (CR). Due to the large difference in concentration levels depending on locations, this analysis perform one analysis for each location. In the interest of keeping the analysis on a moderate level and to get the most reliable analyses, only the locations using at least 10 specimens each year for more than 10 years were used resulting in the 6 locations of Fladen, HarufjÃ¤rden, Landsort, UtlÃ¤ngan, VÃ¤derÃ¶arna and ÃngskÃ¤rsklubb. For the same reasons, only the concentration level in Herring were considered.

In order to justify the use of the Tobit model for the dataset of the museum, an analysis concering whether or not the log-normal distribution holds for the data has to be made. For this purpose, plotting the result from the *cenros* function in the *NADA* packages, as used by Helsel (2005) is one way to go. Since there is no information regarding an exact position for a censored data, only the uncensored data is plotted. Using substitution for the censored data points is of no use since this will result in a different shape of the probability plot dependent on the chosen substitution point. Instead, the proportion of data below each reported limit is calculated and used to fit the uncensored data to the correct quantiles when using a distribution plot. As a result, the uncensored data above the highest reporting limit will have the same positions on the plot as they would have if all data were uncensored. The uncensored values between limits will however be affected by the censored values between these limits, as they should be. Just as bad would be to simply delete the censored values from the data set, using only the uncensored data when plotting a probability plot considering this would skew the percentiles and the distribution will be incorrect. This will also only show the distribution of the uncensored data, not the entire data set. For this thesis, the distribution plot will take the logarithmic values of the concentration and plot against the quantiles of a normal distribution. As can be seen in Figures **Write number of the 3 figures**, in many of the distribution plots, the first point starts around the median or even further to the right. This is the effect of the censored values not being plotted, but at the same time having an impact on the uncensored values position. The *cenros* function by default performs a log-normal transformation prior to operations over the data (Lee, 2017). Furthermore, the transformation back, after operations, is set to *NULL* in order to stop the reverse transformation. Hence, the *cenros* function, with the reverse transformation set to *NULL* assumes a log-normal distribution, and so when using the *cenros* results in a distribution plot, a log-normal distribution assumption is tested. 

In order to estimate the percentiles for the uncensored data, regression on ordered statistics (ROS) is used by the *cenros* function. ROS is favorable over MLE when the proportion of censored data are too high (Helsel, 2005, pp. 86) which is the case in some locations for each metal (see Table 5-7), and for every location for chromium. Each data point is first given a rank $i$ ranking the data point with the smallest values as $i=1$. The ranks are then converted to procentiles by giving each point a plotting position $p$. For the *cenros* function, the position $p$ is given using the Weibull formula $p = i/(n+1)$ where $n$ is the sample size. Even though most commercial statistical softwares use the formula $p = i/n$ (Helsel, 2005, pp.48), the Weibull formula is to be prefered when only using a sample which is a part of the total population. This due to the fact that when using the formula $p=i/n$, it is stated that the largest value has a zero percent chance of being exceeded. This would be the case if the entire population was used, which is rarely the case for environmental studies. When points have the same values and therefor ties in the ranks, as is the case for censored data, each point gets it's own rank. 

When the percentiles are calculated, they are fitted against the quantiles of a normal distribution. The uncensored data is used to calculated the slope and intercept for the linear regression between the logaritmic values of the data and the normal quantiles and thus, fitting this line is fitting a log-normal distribution to the observed data (Helsel, 2005, pp.80). Now, looking at Figure **mention figure numbers**, it seems like a reasonable assumption that the data follows a log-normal distribution seing that they more or less follows a straight line. 


### Applying Tobit model
The data-set was acquired from by Martin SkÃ¶ld from the Swedish Museum of Natural History. The analysis for metals were prior to 2007 performed by the Department of Environmental Assessment at the Swedish University of Agricultural Sciences (SLU). However, from 2007, this was carried out by Department of Environmental
Science and Analytical Chemistry (ACES), at Stockholm University (SU) see Section 6 (Bignert. et. al. 2017). Therefor, the data prior to 2007 and the data from 2007 and forward are not deemed optimal to analyse together. Hence, only the data-set from 2007-2018 is analysed in this thesis in contrast to the report by Bignert. et. al (2017) where both an analysis using all data were used aswell as an analysis for the most recent (2007-2017 in the case of the report) ten years for the longer time series (Section 7, Bignert.et.al. 2017). Hence, an analysis using the Tobit model aswell as the substitution method is made instead of using the result from the report. The data-set contains several variables other than year, location and metal concentration, for example length and weight of the specimens, but only the first three mentioned variables are analysed to follow the line of the published report making it easier to do a good comparison between the two methods. 

The logaritmic values of the reported concentrations, in combination with the coherent year and a vector of censoring indicators, indicating whether or not the observations is censored or not is used with the *lmec* function of the *NADA* package. The proportion of censored values and the standard deviation of the estimated slope is also calculated as well as the standard deviation between-years and the standard deviation for individuals at each location. The standard deviation between-years was calculated using the same method as before with the *cenmle* function from the *NADA* packages. For lead, the standard deviation of each location, each year, had a standard deviation under $0.1$ except for in 2017 at ÃngkÃ¤rsklubben where the standard deviation was around $1.2$. For nickel, the yearly standard deviation was around $0.05-0.2$ through out except for a couple of instances, HarufjÃ¤rden having a year with standard deviation around $1.4$ and ÃngkÃ¤rsklubben having one year with a standard deviation at around $0.7$. For chromium, the censoring proportion is too high to get a sensible estimation of the standard deviation, however, still using the same method, the standard deviation for each year lied around $0.05-0.15$ for the most part, having a couple of year and location combinations with a small increase in standard deviation. One that stood out was the standard deviation at UtlÃ¤ngan in 2008 had a standard deviation over $5$. The standard deviation of the individual specimens for each locations can be seen in Table 5-7.


Due to the fact that the data as used in Bignert. et. al. (2017) and the data for this thesis differs, an analysis is also made using the exact method as would have been used in the report would this data-set have been used, namely the substitution method with a substitution value of $1/\sqrt(2)$ the limit of quantification for the censored value. The results for both methods, including the proportion of censoring and the standard deviation of the location can be seen in Tables **Skriv ut nummer**.
A quick glance of the tables show that the proportion of censored values are mostly different for the three metals. Low to none for all but one location when looking at lead with the anomalous proportion at HarufjÃ¤rden most likely due to the fact that the concentration levels in that location were much lower than in the other locations. For nickel, the proportion is either quite low at $15\%$ censoring or higher at around $50 \%$ censoring while chromium lies above $80 \%$ censoring for all locations. 

Starting the analysis by taking a look at lead (see Table 5) it's possible to see that for each location having low proportion of censored data both models produce similar estimates for the slope as to be expected. However, for HarufjÃ¤rden having a proportion of censoring at around $70 \%$ the same can not be said. The Tobit model produce an estimate of $-0.0215$ on the log-scale implying a yearly decrease in concentration by $2.1 \%$ on the original scale. The substitution model gives an estimate of $0.0301$ implying an increase of $3 \%$ per year. From the simulation study it is shown that the Tobit model produce unbiased estimates when the noise are low as in this case suggesting that the substitution model would misjudge the yearly concentration development severely. Following the information in section 7 of Bignert et. al (2017), a yearly increase of $3 \%$ would imply a doubled concentration level in $24$ years which could commence some sort of action being taken to stop the increase when in fact, a decrease by $2 \%$ mean the concentration level would be halved in $35$ years. 

When looking at nickel (Table 6) the standard deviation for individual specimen increase a small bit, landing somewhere around $0.2-0.35$ except for in HarufjÃ¤rden. As in the case of analysing lead, both models produced similar estimates for the slope for all locations but HarufjÃ¤rden. The Tobit model produced an estimate resulting in an increase of $5.7 \%$ per year on the original scale while substitution showed a yearly decrease by $2.2 \%$. The noise for individual specimens being close to $0.5$ and the between-year noise being around 0.1 for all but one year in combination with $53 \%$ censoring gives an indication on how to judge the difference in estimation for the two models. Looking at Figure **write number: beta plot, medium noise** it's possible to see that for a scenario like this, if the true value of the slope were to be around $0.05$ on the logarithmic scale as estimated by the Tobit model, the majority of the estimates when using the substitution method will be lower than the true value of the slope. The figure also shows that for a higher yearly variance, the amount the substitution method undervaluate the true value increase by a lot. The yearly variance in this scenario may not be as high as in the simulation for all years, but it is higher than the lower level of the study. To summarise, there are indications that if the estimate produced by the Tobit model is correct, it is plausible that the substitution method could give an estimate as low as it did. At the same time, the bias of the Tobit model is larger than that of the substitution method if the true value of the slope would be closer to zero (See Table 3). This due to the fact that a few of the estimates made by the Tobit model is much lower than the truth (see Figure **figure for the betas medium std**). If this is one of those times, or if the substitution method underestimated a large slope is however harder to prove. 

The differences of the estimation for chromium is clear. The simulation study showed that whenever the noise gets lower, the more precise the Tobit model is in contrast to the substitution method. 





```{r}
knitr::include_graphics("Code_Chunks/PB_ros_plots.pdf")
knitr::include_graphics("Code_Chunks/NI_ros_plots.pdf")
knitr::include_graphics("Code_Chunks/CR_ros_plots.pdf")

```



```{r}
kable(for_thesis_PB_df, format = "latex", caption = "Result for both models on data for PB in Herring (Log-Scale)")
kable(for_thesis_NI_df, format = "latex",caption = "Result for both models on data for NI in Herring (Log-Scale)")
kable(for_thesis_CR_df, format = "latex", caption = "Result for both models on data for CR in Herring (Log-Scale)")
```


**Outlier analys?**
## Result

## Conclusion
- EM-algorithm can stop at local maxima or sadle points, at sadle point the LH-fkn grows without bound.
- Can, and should, try with higher (or no) limit of iterations if not so time heavy. 
- Test explicit starting values (based on what?)
- Should have more levels of each factor
- Tobit model seems more consistent in what factors affects estimates in what ways.
- Analysis with negative slopes?


## References
1) Helsel, D.R., 2006, Fabricating data: how substituting values for censored observations can ruin results, and what can be done about it. Chemosphere 65, pp. 2434â2439, doi: https://doi.org/10.1016/j.chemosphere.2006.04.051

2) Chung, C.F., 1990, Regression analysis of geochemical data with observations below detection
limits, in G. Gaal and D.F. Merriam,eds., Computer Applications in Resource Estimation.
Pergammon Press, New York, pp. 421â433, doi: https://doi.org/10.1016/B978-0-08-037245-7.50032-9

3) Lee, T.L and Go, O.T, 1997, Survival Analysis in Public Health Research, vol.18, pp. 105-134, doi: https://doi.org/10.1146/annurev.publhealth.18.1.105

4) Chay, K.Y. and Honore, B.E. , 1998, Estimation of censored semiparametric regression models:
an application to changes in BlackâWhite earnings inequality during the 1960s. Journal of
Human Resources Vol.33, pp. 4â38, doi: 10.2307/146313

5) Pinheiro, J.C and Bates, D.M, (2000), Mixed-Effects Models in S and S-PLUS (1. ed.), New York: Springer  

6) Laird, N. M. and Ware, J. H. (1982). Random-effects models for longitudinal data. *Biometrics*, Vol 38. (No. 4), pp. 963â974., DOI: 10.2307/2529876

7) Bignert, A., Danielsson, S., Faxneld, S., Ek, C., Nyberg, E. (2017). Comments Concerning the National
Swedish Contaminant Monitoring Programme in Marine Biota, 2017, 4:2017, Swedish Museum of Natural
History, Stockholm, Sweden, Retrieved from the website of the Museum of Natural Historys: http://nrm.diva-portal.org/smash/get/diva2:1090746/FULLTEXT01.pdf

8) Eaton, M. L. (1983). Multivariate Statistics: a Vector Space Approach. John Wiley and Sons. pp. 116â117. ISBN 978-0-471-02776-8

9) Held, L, BovÃ©, D.S, (2014), Applied Statistical Inference (1. ed.), New York: Springer

10) Dempster A. P., Laird N. M., Rubin, D. B. (1977), Maximum Likelihood from Incomplete Data via the EM Algorithm. *Journal of the Royal Statistical Society. Series B (Methodological)*, Vol. 39, (No. 1) , pp. 1-38, Retrieved from the website jstor: https://www.jstor.org/stable/2984875?seq=1

11) Thompson, M .L. and Nelson, K. P., (2003), Linear regression with Type I interval- and leftcensored response data. *Environmental and Ecological Statistics* Vol. 10, 221â230. Retrieved from the website of the University of Washington: http://faculty.washington.edu/mlt/Thompson%202003b.pdf

12) El-Shaarawi, A. H., Esterby, S.R.(1992), Replacement of censored observations by a constant: An evaluation. *Water Research*, Vol 26. (No. 6), pp. 835-844, doi: https://doi.org/10.1016/0043-1354(92)90015-V

13) Helsel, D.R., (2005), STATISTICS FOR CENSORED ENVIRONMENTAL DATA USING MINITAB AND R (2. ed.), Hoboken, New Jersey: John Wiley & Sons, pp. 62-69, Inc., ISBN 978-0-470-47988-9(cloth) 

14) Vaida, F., Liu, L. (2009), Fast Implementation for Normal Mixed Effects Models With Censored Response. *Journal of Computational and Graphical Statistics* 
Vol 18. (No. 4), 2009 - Issue 4 , doi: https://doi.org/10.1198/jcgs.2009.07130

15) Lee. L (2017). NADA: Nondetects and Data Analysis for Environmental Data. R package version 1.6-1. https://CRAN.R-project.org/package=NADA